{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 97849,
          "databundleVersionId": 11656167,
          "sourceType": "competition"
        },
        {
          "sourceId": 11637102,
          "sourceType": "datasetVersion",
          "datasetId": 7301642
        },
        {
          "sourceId": 11681351,
          "sourceType": "datasetVersion",
          "datasetId": 7331502
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/polinak1r/Dense-Retrieval-with-BERT/blob/main/Dense_Retrieval_with_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense Retrieval with BERT\n",
        "This notebook implements a dense retrieval pipeline for semantic search using a pretrained RoBERTa model.  \n",
        "\n",
        "1. **Data Preparation**  \n",
        "   Load data. For quick experiments apply sampling.\n",
        "\n",
        "2. **Model Initialization and Embedding Computation**  \n",
        "   Load and explore RoBERTa from HuggingFace Transformers. Compute contextual embeddings for queries and documents via masked mean pooling over token-level outputs, followed by batch standardization and L2 normalization.\n",
        "\n",
        "3. **Similarity Scoring and Retrieval Evaluation**  \n",
        "   Compute pairwise similarities between query and document embeddings using dot product. Rank documents per query and evaluate retrieval performance using the PFound metric, which reflects ranked relevance under user browsing behavior.\n",
        "\n",
        "4. **Inference and Submission**  \n",
        "   Run the trained pipeline on test queries, generate predicted rankings, and export results in kaggle submission format.\n"
      ],
      "metadata": {
        "id": "0cfy7tXMIkLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data downloading"
      ],
      "metadata": {
        "id": "HbA_9rxTF3mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "data_dir = Path('/kaggle/input/nlp-nup-2025-hw2/')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:53:52.373689Z",
          "iopub.execute_input": "2025-05-05T13:53:52.374065Z",
          "iopub.status.idle": "2025-05-05T13:53:57.229893Z",
          "shell.execute_reply.started": "2025-05-05T13:53:52.374034Z",
          "shell.execute_reply": "2025-05-05T13:53:57.228613Z"
        },
        "id": "QZKcChLrF3mp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "with open(data_dir / 'documents.jsonl') as fp:\n",
        "    for line in tqdm(fp, total=367840):\n",
        "        docs.append(json.loads(line))\n",
        "\n",
        "with open(data_dir / 'queries_train.json') as fp:\n",
        "    queries = json.load(fp)\n",
        "\n",
        "with open(data_dir / 'qrels_train.json') as fp:\n",
        "    qrels = json.load(fp)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:09:08.931135Z",
          "iopub.execute_input": "2025-05-05T13:09:08.931422Z",
          "iopub.status.idle": "2025-05-05T13:09:14.785982Z",
          "shell.execute_reply.started": "2025-05-05T13:09:08.931401Z",
          "shell.execute_reply": "2025-05-05T13:09:14.785300Z"
        },
        "id": "oSQj1UbwF3mq",
        "outputId": "8d1c6f54-62a9-4a43-a8ee-2e69e316bd70"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 367840/367840 [00:05<00:00, 63692.30it/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reduced sample (1:10 positive and negative) for quick experiments"
      ],
      "metadata": {
        "id": "DBQP002mF3mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "count = 10  # number of negative examples per every positive example (1:10 now)\n",
        "seed = 42\n",
        "\n",
        "pos_doc_ids = {\n",
        "    rec['doc_id']\n",
        "    for rec in qrels\n",
        "    if rec.get('relevance', 0) > 0\n",
        "}\n",
        "all_doc_ids = {doc['id'] for doc in docs}\n",
        "\n",
        "random.seed(seed)\n",
        "neg_sample = set(random.sample(list(all_doc_ids - pos_doc_ids), count * len(pos_doc_ids)))\n",
        "\n",
        "docs = [doc for doc in docs if doc['id'] in pos_doc_ids or doc['id'] in neg_sample]\n",
        "print(f'Filtered docs length {len(docs)}')\n",
        "\n",
        "qrels = [rec for rec in qrels if rec['doc_id'] in pos_doc_ids or rec['doc_id'] in neg_sample]\n",
        "print(f'Filtered qrels length {len(qrels)}')\n",
        "\n",
        "valid_query_ids = {rec['query_id'] for rec in qrels}\n",
        "queries = [q for q in queries if q['query_id'] in valid_query_ids]\n",
        "print(f'Filtered queries length {len(queries)}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:09:18.655539Z",
          "iopub.execute_input": "2025-05-05T13:09:18.655857Z",
          "iopub.status.idle": "2025-05-05T13:09:19.440472Z",
          "shell.execute_reply.started": "2025-05-05T13:09:18.655831Z",
          "shell.execute_reply": "2025-05-05T13:09:19.439620Z"
        },
        "id": "MsNcRcYTF3mq",
        "outputId": "07733613-02e0-4f42-fb0c-801d1fbb3426"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Filtered docs length 26664\nFiltered qrels length 2711\nFiltered queries length 28\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT model"
      ],
      "metadata": {
        "id": "qPZrfQqIF3mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model = AutoModel.from_pretrained('roberta-large')\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-large')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:13:57.177840Z",
          "iopub.execute_input": "2025-05-05T13:13:57.178171Z",
          "iopub.status.idle": "2025-05-05T13:14:25.507259Z",
          "shell.execute_reply.started": "2025-05-05T13:13:57.178142Z",
          "shell.execute_reply": "2025-05-05T13:14:25.506312Z"
        },
        "colab": {
          "referenced_widgets": [
            "90a026edcc5e4e048812e54ed7bc5373",
            "3a05a263b9df4458bc645e0d86d7043e",
            "721a9e35482343e9bbc9856e9d6c77bf",
            "03dbe3eefff945c792bd6b293da6604b",
            "36e7282f86234572a6b512a749d2f29a",
            "c543c169e7864a2ea10ec7d0ccceb199"
          ]
        },
        "id": "0ur0l-JDF3mr",
        "outputId": "4c3788d2-1a88-492c-e6dc-83e767f71707"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90a026edcc5e4e048812e54ed7bc5373"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a05a263b9df4458bc645e0d86d7043e"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "721a9e35482343e9bbc9856e9d6c77bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03dbe3eefff945c792bd6b293da6604b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36e7282f86234572a6b512a749d2f29a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c543c169e7864a2ea10ec7d0ccceb199"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:14:25.508523Z",
          "iopub.execute_input": "2025-05-05T13:14:25.509291Z",
          "iopub.status.idle": "2025-05-05T13:14:25.513938Z",
          "shell.execute_reply.started": "2025-05-05T13:14:25.509267Z",
          "shell.execute_reply": "2025-05-05T13:14:25.513218Z"
        },
        "id": "3Ch0OwxAF3mr",
        "outputId": "9a7ed29e-a1a8-4069-8036-d134084e12c2"
      },
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "RobertaTokenizerFast(name_or_path='roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n}\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:14:25.515336Z",
          "iopub.execute_input": "2025-05-05T13:14:25.515604Z",
          "iopub.status.idle": "2025-05-05T13:14:25.521402Z",
          "shell.execute_reply.started": "2025-05-05T13:14:25.515584Z",
          "shell.execute_reply": "2025-05-05T13:14:25.520848Z"
        },
        "id": "vYUknqfkF3mr",
        "outputId": "872b75ef-9693-4b0b-cd12-bcb45fc7d385"
      },
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 1024)\n    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0-23): 24 x RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSdpaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (activation): Tanh()\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "example = queries[0]['query']\n",
        "print(example)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:14:47.485474Z",
          "iopub.execute_input": "2025-05-05T13:14:47.485861Z",
          "iopub.status.idle": "2025-05-05T13:14:47.490181Z",
          "shell.execute_reply.started": "2025-05-05T13:14:47.485829Z",
          "shell.execute_reply": "2025-05-05T13:14:47.489480Z"
        },
        "id": "wADbUUHXF3mr",
        "outputId": "9e8f52aa-fb4b-406f-bfb0-488008233713"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Would the United Kingdom have been ready for WWII without the time gained through Appeasement?\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(example)\n",
        "print(input_ids)\n",
        "print(len(input_ids))\n",
        "print()\n",
        "print(tokenizer.added_tokens_decoder)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:14:49.897860Z",
          "iopub.execute_input": "2025-05-05T13:14:49.898173Z",
          "iopub.status.idle": "2025-05-05T13:14:49.910037Z",
          "shell.execute_reply.started": "2025-05-05T13:14:49.898145Z",
          "shell.execute_reply": "2025-05-05T13:14:49.909316Z"
        },
        "id": "1iavbpliF3mr",
        "outputId": "add44b5e-cf84-4ab6-c215-cb8ae93d7d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[0, 29042, 5, 315, 5752, 33, 57, 1227, 13, 29001, 396, 5, 86, 3491, 149, 3166, 29358, 6285, 116, 2]\n20\n\n{0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True)}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_example = tokenizer.decode(input_ids)\n",
        "print(decoded_example)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:14:52.194664Z",
          "iopub.execute_input": "2025-05-05T13:14:52.195009Z",
          "iopub.status.idle": "2025-05-05T13:14:52.199354Z",
          "shell.execute_reply.started": "2025-05-05T13:14:52.194977Z",
          "shell.execute_reply": "2025-05-05T13:14:52.198691Z"
        },
        "id": "l0UrsN3wF3mr",
        "outputId": "e0a432b0-226a-4339-94cd-39442c3fd255"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<s>Would the United Kingdom have been ready for WWII without the time gained through Appeasement?</s>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(example, return_tensors='pt')\n",
        "input_ids = inputs.input_ids\n",
        "print(inputs)\n",
        "print()\n",
        "print()\n",
        "print(input_ids)\n",
        "print(input_ids.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T10:25:54.872787Z",
          "iopub.execute_input": "2025-05-01T10:25:54.873086Z",
          "iopub.status.idle": "2025-05-01T10:25:54.887813Z",
          "shell.execute_reply.started": "2025-05-01T10:25:54.873054Z",
          "shell.execute_reply": "2025-05-01T10:25:54.886968Z"
        },
        "id": "2YS0-puMF3mr",
        "outputId": "822dd9a3-18a6-47e8-bf66-d31ec976f676"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'input_ids': tensor([[    0, 29042,     5,   315,  5752,    33,    57,  1227,    13, 29001,\n           396,     5,    86,  3491,   149,  3166, 29358,  6285,   116,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n\ntensor([[    0, 29042,     5,   315,  5752,    33,    57,  1227,    13, 29001,\n           396,     5,    86,  3491,   149,  3166, 29358,  6285,   116,     2]])\ntorch.Size([1, 20])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the padding tokens in the second sample"
      ],
      "metadata": {
        "id": "CjxodD1IF3mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer([example, example[:10]], return_tensors='pt', truncation=True, padding=True)\n",
        "input_ids = inputs.input_ids\n",
        "print(inputs)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:14:56.333972Z",
          "iopub.execute_input": "2025-05-05T13:14:56.334307Z",
          "iopub.status.idle": "2025-05-05T13:14:56.346934Z",
          "shell.execute_reply.started": "2025-05-05T13:14:56.334267Z",
          "shell.execute_reply": "2025-05-05T13:14:56.346299Z"
        },
        "id": "tmWJLTGyF3ms",
        "outputId": "efb52122-2010-40eb-cdc3-a75c6f69fc58"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'input_ids': tensor([[    0, 29042,     5,   315,  5752,    33,    57,  1227,    13, 29001,\n           396,     5,    86,  3491,   149,  3166, 29358,  6285,   116,     2],\n        [    0, 29042,     5,  1437,     2,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_result = model(input_ids)\n",
        "print(model_result.keys())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:14:59.394240Z",
          "iopub.execute_input": "2025-05-05T13:14:59.394595Z",
          "iopub.status.idle": "2025-05-05T13:14:59.950182Z",
          "shell.execute_reply.started": "2025-05-05T13:14:59.394563Z",
          "shell.execute_reply": "2025-05-05T13:14:59.949162Z"
        },
        "id": "xfHp7WYtF3ms",
        "outputId": "a444f2e1-84cb-4877-f8e5-802c01dac6e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "odict_keys(['last_hidden_state', 'pooler_output'])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_result.last_hidden_state)\n",
        "print()\n",
        "print(model_result.last_hidden_state.shape)\n",
        "print(model_result.last_hidden_state[0, :, :].shape)\n",
        "print(model_result.last_hidden_state[:, 0, :].shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:15:02.218323Z",
          "iopub.execute_input": "2025-05-05T13:15:02.218690Z",
          "iopub.status.idle": "2025-05-05T13:15:02.247451Z",
          "shell.execute_reply.started": "2025-05-05T13:15:02.218621Z",
          "shell.execute_reply": "2025-05-05T13:15:02.246815Z"
        },
        "id": "1FYauas-F3ms",
        "outputId": "02d98643-6cbd-4d2b-bab9-540660cb1356"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor([[[-0.1099,  0.0019, -0.0124,  ..., -0.0764,  0.0463,  0.1033],\n         [ 0.0377,  0.2558, -0.6451,  ..., -0.0571, -0.1688, -0.3992],\n         [-0.0200,  0.0879, -0.0725,  ...,  0.0603, -0.0499,  0.1073],\n         ...,\n         [-0.0410,  0.0792, -0.2168,  ..., -0.2132, -0.0974, -0.1989],\n         [-0.3018, -0.4035, -0.3044,  ...,  0.1915, -0.1838,  0.3145],\n         [-0.0610, -0.0411, -0.0086,  ..., -0.1118, -0.0074,  0.0444]],\n\n        [[-0.0327, -0.0315, -0.0383,  ..., -0.0288,  0.1306,  0.0035],\n         [-0.0035,  0.0217, -0.1948,  ...,  0.2103, -0.0253,  0.0205],\n         [-0.0670,  0.1362, -0.1744,  ...,  0.1401,  0.1191,  0.0968],\n         ...,\n         [-0.0249,  0.1890, -0.1642,  ..., -0.1527,  0.0173,  0.1446],\n         [-0.0249,  0.1890, -0.1642,  ..., -0.1527,  0.0173,  0.1446],\n         [-0.0249,  0.1890, -0.1642,  ..., -0.1527,  0.0173,  0.1446]]],\n       grad_fn=<NativeLayerNormBackward0>)\n\ntorch.Size([2, 20, 1024])\ntorch.Size([20, 1024])\ntorch.Size([2, 1024])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_result.pooler_output)\n",
        "print(model_result.pooler_output.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:15:05.975115Z",
          "iopub.execute_input": "2025-05-05T13:15:05.975396Z",
          "iopub.status.idle": "2025-05-05T13:15:05.980509Z",
          "shell.execute_reply.started": "2025-05-05T13:15:05.975374Z",
          "shell.execute_reply": "2025-05-05T13:15:05.979894Z"
        },
        "id": "jsY4VFD8F3ms",
        "outputId": "a9092db4-a159-4f36-8bf3-fab3216f2df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor([[ 0.3217, -0.3042, -0.5720,  ..., -0.3693,  0.3369, -0.5884],\n        [ 0.2873, -0.3008, -0.5220,  ..., -0.3009,  0.4818, -0.6207]],\n       grad_fn=<TanhBackward0>)\ntorch.Size([2, 1024])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.pooler"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T10:26:15.816399Z",
          "iopub.execute_input": "2025-05-01T10:26:15.816693Z",
          "iopub.status.idle": "2025-05-01T10:26:15.821981Z",
          "shell.execute_reply.started": "2025-05-01T10:26:15.816668Z",
          "shell.execute_reply": "2025-05-01T10:26:15.821225Z"
        },
        "id": "Dwrpc54NF3ms",
        "outputId": "06799f03-8d5e-453b-d512-fe85dfeb1630"
      },
      "outputs": [
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "RobertaPooler(\n  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n  (activation): Tanh()\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pooled_output = model.pooler.dense(model_result.last_hidden_state[:, 0])\n",
        "pooled_output = model.pooler.activation(pooled_output)\n",
        "torch.equal(pooled_output, model_result.pooler_output)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:15:11.063268Z",
          "iopub.execute_input": "2025-05-05T13:15:11.063562Z",
          "iopub.status.idle": "2025-05-05T13:15:11.069078Z",
          "shell.execute_reply.started": "2025-05-05T13:15:11.063539Z",
          "shell.execute_reply": "2025-05-05T13:15:11.068436Z"
        },
        "id": "Lup6YkEyF3ms",
        "outputId": "5cf30d2f-fb28-4bcb-eecd-7cd3a55ca92f"
      },
      "outputs": [
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Dataset and DataLoader\n",
        "The pipeline of text preprocessing for neural networks is the following:\n",
        "- tokenization, note that special tokens are often automatically added at this step, in our case, `[CLS]` at the beginning and `[SEP]` at the end. This is `Dataset` part in our implementation.\n",
        "- batching, at this step, we concatenate multiple token indices sequences into a matrix of the shape `batch_size x longest_seq_len`. Important, we pad sequences in the batch with `seq_len < longest_seq_len` with special token `[PAD]` up to the `longest_seq_len` (in our implementation, this is done under-the-hood of tokenizer). The model should not attend to these tokens, and for this purpose `attention_mask` of the shape `batch_size x longest_seq_len` is generated by tokenizer and passed to the model. This is `DataLoader` part in our implementation.\n",
        "\n",
        "In the current implementation, tokenization is performed on the fly while we iterate over DataLoader. An alternative approach is to first tokenize the data in the `Dataset` class. Tokenization in `Dataset` has a potential advantage: it is performed once and does not take extra time in the case when we want to perform inference multiple times on the same data, e.g. during training multiple epochs. However, the advantage can be minimal if we have enough CPU cores, since DataLoader can be easily parallelized with `num_workers > 1` input parameter."
      ],
      "metadata": {
        "id": "EWWD-ItdF3ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DocsDataset(Dataset):\n",
        "    def __init__(self, docs, char_max_length=8192):\n",
        "        '''\n",
        "        char_max_length: int\n",
        "            Maximum number of characters to keep from each document;\n",
        "            Used to control the speed of tokenization\n",
        "            (tokenization of the full document might be too time consuming)\n",
        "        '''\n",
        "        # self.docs_full = [doc['title'][:char_max_length] for doc in docs]\n",
        "        self.docs_full = [ (doc['title'] + ' ' + doc['contents'])[:char_max_length] for doc in docs]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.docs_full)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.docs_full[idx]\n",
        "\n",
        "\n",
        "class QueriesDataset(Dataset):\n",
        "    def __init__(self, queries, char_max_length=8192):\n",
        "        '''\n",
        "        char_max_length: int\n",
        "            Maximum number of characters to keep from each query;\n",
        "            Used to control the speed of tokenization\n",
        "            (tokenization of the full document might be too time consuming)\n",
        "        '''\n",
        "        self.queries_full = [query['query'][:char_max_length] for query in queries]\n",
        "        # self.queries_full = [(query['query']  + ' ' + query['guidelines'])[:char_max_length] for query in queries]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries_full)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.queries_full[idx]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:35:15.569515Z",
          "iopub.execute_input": "2025-05-05T13:35:15.569854Z",
          "iopub.status.idle": "2025-05-05T13:35:15.575298Z",
          "shell.execute_reply.started": "2025-05-05T13:35:15.569825Z",
          "shell.execute_reply": "2025-05-05T13:35:15.574634Z"
        },
        "id": "lAtP9JLTF3ms"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "docs_dataset = DocsDataset(docs, char_max_length=256)\n",
        "queries_dataset = QueriesDataset(queries, char_max_length=128)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:35:18.715212Z",
          "iopub.execute_input": "2025-05-05T13:35:18.715507Z",
          "iopub.status.idle": "2025-05-05T13:35:18.857529Z",
          "shell.execute_reply.started": "2025-05-05T13:35:18.715484Z",
          "shell.execute_reply": "2025-05-05T13:35:18.856880Z"
        },
        "id": "96tVbNDKF3ms"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs_dataset), len(queries_dataset)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:35:20.592117Z",
          "iopub.execute_input": "2025-05-05T13:35:20.592432Z",
          "iopub.status.idle": "2025-05-05T13:35:20.597118Z",
          "shell.execute_reply.started": "2025-05-05T13:35:20.592405Z",
          "shell.execute_reply": "2025-05-05T13:35:20.596366Z"
        },
        "id": "_vncyD7fF3mt",
        "outputId": "923f2fb2-cc09-48ef-af18-2b76193217ae"
      },
      "outputs": [
        {
          "execution_count": 56,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(26664, 28)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch: list[str], token_max_length: int):\n",
        "    tokenized_batch = tokenizer(\n",
        "        batch,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=token_max_length,\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    )\n",
        "    return tokenized_batch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:35:28.651717Z",
          "iopub.execute_input": "2025-05-05T13:35:28.652015Z",
          "iopub.status.idle": "2025-05-05T13:35:28.655554Z",
          "shell.execute_reply.started": "2025-05-05T13:35:28.651992Z",
          "shell.execute_reply": "2025-05-05T13:35:28.654758Z"
        },
        "id": "xMWd1Y6sF3mt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the number of processors (cores) with terminal command to make batching mutltiprocess"
      ],
      "metadata": {
        "id": "jAU7LYDvF3mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nproc"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:20:47.432523Z",
          "iopub.execute_input": "2025-05-05T13:20:47.432845Z",
          "iopub.status.idle": "2025-05-05T13:20:47.652815Z",
          "shell.execute_reply.started": "2025-05-05T13:20:47.432818Z",
          "shell.execute_reply": "2025-05-05T13:20:47.651761Z"
        },
        "id": "uMT2W3fTF3mt",
        "outputId": "aafdb3b0-dbd5-4cf9-9578-bdf69f1586bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "4\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import multiprocessing\n",
        "from functools import partial\n",
        "\n",
        "batch_size = 512\n",
        "num_workers = multiprocessing.cpu_count() # 4\n",
        "token_max_length = 128\n",
        "\n",
        "final_collate_fn = partial(collate_fn,\n",
        "                           token_max_length=token_max_length)\n",
        "\n",
        "docs_dataloader = DataLoader(\n",
        "    docs_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=False,\n",
        "    collate_fn=final_collate_fn,\n",
        ")\n",
        "queries_dataloader = DataLoader(\n",
        "    queries_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=False,\n",
        "    collate_fn=final_collate_fn,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:35:30.913300Z",
          "iopub.execute_input": "2025-05-05T13:35:30.913592Z",
          "iopub.status.idle": "2025-05-05T13:35:30.919370Z",
          "shell.execute_reply.started": "2025-05-05T13:35:30.913570Z",
          "shell.execute_reply": "2025-05-05T13:35:30.918516Z"
        },
        "id": "D9zMkaRfF3mt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(docs_dataloader))['input_ids'].shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:35:37.389982Z",
          "iopub.execute_input": "2025-05-05T13:35:37.390270Z",
          "iopub.status.idle": "2025-05-05T13:35:38.225003Z",
          "shell.execute_reply.started": "2025-05-05T13:35:37.390249Z",
          "shell.execute_reply": "2025-05-05T13:35:38.224125Z"
        },
        "id": "g84rhH4gF3mt",
        "outputId": "ddb1c681-e91e-4d99-d64e-7fb7bbf2e170"
      },
      "outputs": [
        {
          "execution_count": 61,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([512, 98])"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(docs_dataloader))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:35:39.150839Z",
          "iopub.execute_input": "2025-05-05T13:35:39.151162Z",
          "iopub.status.idle": "2025-05-05T13:35:40.017509Z",
          "shell.execute_reply.started": "2025-05-05T13:35:39.151136Z",
          "shell.execute_reply": "2025-05-05T13:35:40.016619Z"
        },
        "id": "MslfwNcpF3mt",
        "outputId": "40c19b47-04a6-4039-c0d8-4161400a696e"
      },
      "outputs": [
        {
          "execution_count": 62,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'input_ids': tensor([[    0, 13365,   208,  ...,     1,     1,     1],\n        [    0,   597,  2685,  ...,     1,     1,     1],\n        [    0,  5320,  6368,  ...,     1,     1,     1],\n        ...,\n        [    0,  4741, 22471,  ...,     1,     1,     1],\n        [    0,  8773,  5075,  ...,     1,     1,     1],\n        [    0, 11773,    18,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating embeddings"
      ],
      "metadata": {
        "id": "9tttz33AF3mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_embeddings(model, dataloader: DataLoader, device='cuda') -> torch.Tensor:\n",
        "    all_embeddings = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            # masked mean pooling\n",
        "            last_hidden = outputs.last_hidden_state\n",
        "            mask = attention_mask.unsqueeze(-1)\n",
        "            masked_hidden = last_hidden * mask\n",
        "            sum_hidden = masked_hidden.sum(dim=1)\n",
        "            lengths = mask.sum(dim=1)\n",
        "            embeddings = sum_hidden / (lengths + 1e-6)\n",
        "\n",
        "            # Batch standardization\n",
        "            mean = embeddings.mean(dim=0, keepdim=True)\n",
        "            std = embeddings.std(dim=0, keepdim=True)\n",
        "            embeddings = embeddings / (std + 1e-6)\n",
        "\n",
        "            # L2 norm\n",
        "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "            all_embeddings.append(embeddings.cpu())\n",
        "\n",
        "    return torch.cat(all_embeddings, dim=0)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:35:42.852435Z",
          "iopub.execute_input": "2025-05-05T13:35:42.852781Z",
          "iopub.status.idle": "2025-05-05T13:35:42.858846Z",
          "shell.execute_reply.started": "2025-05-05T13:35:42.852750Z",
          "shell.execute_reply": "2025-05-05T13:35:42.858121Z"
        },
        "id": "LrUiBvLSF3mw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:35:45.739566Z",
          "iopub.execute_input": "2025-05-05T13:35:45.739877Z",
          "iopub.status.idle": "2025-05-05T13:35:45.749164Z",
          "shell.execute_reply.started": "2025-05-05T13:35:45.739855Z",
          "shell.execute_reply": "2025-05-05T13:35:45.748542Z"
        },
        "id": "htC9mVMuF3mw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating document embeddings...\")\n",
        "doc_embeddings = get_embeddings(model, docs_dataloader)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:35:47.485728Z",
          "iopub.execute_input": "2025-05-05T13:35:47.486021Z",
          "iopub.status.idle": "2025-05-05T13:45:17.504771Z",
          "shell.execute_reply.started": "2025-05-05T13:35:47.486000Z",
          "shell.execute_reply": "2025-05-05T13:45:17.503728Z"
        },
        "id": "fw7WQ5mTF3mw",
        "outputId": "aba6bf84-be89-45f1-e4c9-0d913d2af0c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Generating document embeddings...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Generating embeddings: 100%|██████████| 53/53 [09:29<00:00, 10.75s/it]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating query embeddings...\")\n",
        "query_embeddings = get_embeddings(model, queries_dataloader)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:46:46.875710Z",
          "iopub.execute_input": "2025-05-05T13:46:46.876060Z",
          "iopub.status.idle": "2025-05-05T13:46:47.430635Z",
          "shell.execute_reply.started": "2025-05-05T13:46:46.876032Z",
          "shell.execute_reply": "2025-05-05T13:46:47.429749Z"
        },
        "id": "4G8gfoGMF3mw",
        "outputId": "e3cccdc8-2126-4671-a323-d1f0135c3ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Generating query embeddings...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embeddings.shape, query_embeddings.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:46:48.762413Z",
          "iopub.execute_input": "2025-05-05T13:46:48.762752Z",
          "iopub.status.idle": "2025-05-05T13:46:48.767867Z",
          "shell.execute_reply.started": "2025-05-05T13:46:48.762723Z",
          "shell.execute_reply": "2025-05-05T13:46:48.767164Z"
        },
        "id": "7F4tczz7F3mx",
        "outputId": "6dc01a84-7f06-41a1-d00c-1f7a7d48892f"
      },
      "outputs": [
        {
          "execution_count": 68,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([26664, 1024]), torch.Size([28, 1024]))"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing similarity"
      ],
      "metadata": {
        "id": "vv06YQHqF3mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_euclidean(doc_embedding, query_embedding) -> float:\n",
        "    return float(-torch.norm(doc_embedding - query_embedding).item()) # euclidean similarity"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T13:36:11.703200Z",
          "iopub.execute_input": "2025-05-01T13:36:11.703520Z",
          "iopub.status.idle": "2025-05-01T13:36:11.707610Z",
          "shell.execute_reply.started": "2025-05-01T13:36:11.703497Z",
          "shell.execute_reply": "2025-05-01T13:36:11.706639Z"
        },
        "id": "NZHKCbWvF3mx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_L1(doc_embedding, query_embedding) -> float:\n",
        "    return float(-torch.sum(torch.abs(doc_embedding - query_embedding)).item()) # L1 similarity"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T16:29:40.376512Z",
          "iopub.execute_input": "2025-05-01T16:29:40.376880Z",
          "iopub.status.idle": "2025-05-01T16:29:40.381729Z",
          "shell.execute_reply.started": "2025-05-01T16:29:40.376848Z",
          "shell.execute_reply": "2025-05-01T16:29:40.380671Z"
        },
        "id": "OW3_3Ip7F3mx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(doc_embedding, query_embedding) -> float:\n",
        "    return float(doc_embedding @ query_embedding.T) # dot product similarity"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:26:48.349265Z",
          "iopub.execute_input": "2025-05-05T13:26:48.349616Z",
          "iopub.status.idle": "2025-05-05T13:26:48.353128Z",
          "shell.execute_reply.started": "2025-05-05T13:26:48.349586Z",
          "shell.execute_reply": "2025-05-05T13:26:48.352421Z"
        },
        "id": "XvU1Rh4bF3mx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "# this is a common way to import `functional` module\n",
        "# however, one of the creators of the HW2 thinks that it\n",
        "# is way more beautiful to write\n",
        "# import torch.nn.functional as tofu\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i, q in enumerate(query_embeddings):\n",
        "    for j, d in enumerate(tqdm(doc_embeddings)):\n",
        "        pred_sim = similarity(d, q)\n",
        "        preds.append({\n",
        "            'doc_id': docs[j]['id'],\n",
        "            'query_id': queries[i]['query_id'],\n",
        "            'score': pred_sim\n",
        "        })"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:46:56.176577Z",
          "iopub.execute_input": "2025-05-05T13:46:56.176910Z",
          "iopub.status.idle": "2025-05-05T13:47:04.845765Z",
          "shell.execute_reply.started": "2025-05-05T13:46:56.176885Z",
          "shell.execute_reply": "2025-05-05T13:47:04.845071Z"
        },
        "id": "dtUSAghuF3mx",
        "outputId": "72634ddd-0e77-4b97-dbf5-97c5c399d8cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 26664/26664 [00:00<00:00, 88270.33it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 109128.46it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 105462.95it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 107035.62it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 108751.32it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 41197.32it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 102308.80it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 110094.35it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 109209.77it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 109579.69it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 109903.18it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 111260.81it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 106340.30it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 45667.97it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 105198.38it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 101400.30it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 108212.45it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 109152.53it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 107897.37it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 105482.55it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 45504.45it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 109921.21it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 111398.68it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 109183.01it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 107558.15it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 107425.49it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 106342.12it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 44145.24it/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def pfound_score(y_true: 'npt.NDArray[np.int_]', y_score: 'npt.NDArray[np.float_]', pbreak: float = .15) -> float:\n",
        "    assert y_true.shape == y_score.shape\n",
        "\n",
        "    indices = np.argsort(y_score)[::-1]\n",
        "\n",
        "    y_max = max(y_true)\n",
        "\n",
        "    pfound, plook = 0., 1.\n",
        "\n",
        "    for rank, i in enumerate(indices):\n",
        "        r = (2. ** y_true[i] - 1.) / (2. ** y_max)\n",
        "\n",
        "        pfound += r * plook * pbreak ** rank\n",
        "\n",
        "        plook *= 1. - r\n",
        "\n",
        "    return pfound\n",
        "\n",
        "\n",
        "def pfound(qrels_list: list[dict[str: str | int]],\n",
        "           y_pred: list[dict[str: str | float]],\n",
        "           pbreak: float = 0.15\n",
        "          ) -> float:\n",
        "    assert 0 < pbreak < 1\n",
        "    zero_score_qrel = {'score': 0.0, 'relevance': 0.0}\n",
        "\n",
        "    queries = set(qrel['query_id'] for qrel in qrels_list)\n",
        "    p_found_list = []\n",
        "    for cur_query in queries:\n",
        "        cur_y_pred_dicts = [doc_ranked for doc_ranked in y_pred\n",
        "                            if doc_ranked['query_id'] == cur_query]\n",
        "        y = {qrel['doc_id']: qrel for qrel in qrels_list if qrel['query_id'] == cur_query}\n",
        "        cur_y_pred = np.empty(len(cur_y_pred_dicts))\n",
        "        cur_y_true = np.empty(len(cur_y_pred_dicts))\n",
        "        for n, y_pred_dict in enumerate(cur_y_pred_dicts):\n",
        "            cur_y_pred[n] = y_pred_dict['score']\n",
        "            cur_y_true[n] = y.get(y_pred_dict['doc_id'], zero_score_qrel)['relevance']\n",
        "\n",
        "        cur_pfound = pfound_score(np.array(cur_y_true), np.array(cur_y_pred))\n",
        "        p_found_list.append(cur_pfound)\n",
        "    return float(np.mean(p_found_list))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:47:07.277071Z",
          "iopub.execute_input": "2025-05-05T13:47:07.277356Z",
          "iopub.status.idle": "2025-05-05T13:47:07.284180Z",
          "shell.execute_reply.started": "2025-05-05T13:47:07.277334Z",
          "shell.execute_reply": "2025-05-05T13:47:07.283412Z"
        },
        "id": "BTS3uSEJF3mx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pfound(qrels, preds)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:47:09.921741Z",
          "iopub.execute_input": "2025-05-05T13:47:09.922037Z",
          "iopub.status.idle": "2025-05-05T13:47:12.357603Z",
          "shell.execute_reply.started": "2025-05-05T13:47:09.922015Z",
          "shell.execute_reply": "2025-05-05T13:47:12.356715Z"
        },
        "id": "TzqAcqHvF3mx",
        "outputId": "ae492d34-4748-4cd1-86f9-072d8e007cd9"
      },
      "outputs": [
        {
          "execution_count": 71,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.3733633557789204"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [p['score'] for p in preds]\n",
        "print(min(scores), max(scores), sum(scores)/len(scores))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:47:14.510842Z",
          "iopub.execute_input": "2025-05-05T13:47:14.511130Z",
          "iopub.status.idle": "2025-05-05T13:47:14.592864Z",
          "shell.execute_reply.started": "2025-05-05T13:47:14.511107Z",
          "shell.execute_reply": "2025-05-05T13:47:14.591973Z"
        },
        "id": "ytZaO9rXF3mx",
        "outputId": "c878c3fc-32c5-4222-c301-afa49e1e1281"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "0.7843459248542786 0.9780784845352173 0.9310470349293283\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "We use the same database of documents, but will load test set queries."
      ],
      "metadata": {
        "id": "cRr3WlEaF3mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_dir / 'queries_test.json') as fp:\n",
        "    qs_test = json.load(fp)\n",
        "\n",
        "print(f'Number of queries: {len(qs_test)}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:47:40.069220Z",
          "iopub.execute_input": "2025-05-05T13:47:40.069563Z",
          "iopub.status.idle": "2025-05-05T13:47:40.089543Z",
          "shell.execute_reply.started": "2025-05-05T13:47:40.069531Z",
          "shell.execute_reply": "2025-05-05T13:47:40.088902Z"
        },
        "id": "ZVH6ImQlF3mx",
        "outputId": "2b67c4c8-4353-4be9-c9ed-2d364f4142b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of queries: 14\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "qs_test_dataset = QueriesDataset(qs_test)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:47:48.973018Z",
          "iopub.execute_input": "2025-05-05T13:47:48.973308Z",
          "iopub.status.idle": "2025-05-05T13:47:48.976460Z",
          "shell.execute_reply.started": "2025-05-05T13:47:48.973285Z",
          "shell.execute_reply": "2025-05-05T13:47:48.975687Z"
        },
        "id": "39RcGnz_F3mx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "num_workers = multiprocessing.cpu_count() # 4\n",
        "token_max_length = 32\n",
        "\n",
        "final_collate_fn = partial(collate_fn,\n",
        "                           token_max_length=token_max_length)\n",
        "\n",
        "qs_test_dataloader = DataLoader(\n",
        "    qs_test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=False,\n",
        "    collate_fn=final_collate_fn,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:47:52.772496Z",
          "iopub.execute_input": "2025-05-05T13:47:52.772826Z",
          "iopub.status.idle": "2025-05-05T13:47:52.776688Z",
          "shell.execute_reply.started": "2025-05-05T13:47:52.772796Z",
          "shell.execute_reply": "2025-05-05T13:47:52.775991Z"
        },
        "id": "-OAkSNwsF3mx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating query embeddings...\")\n",
        "qs_test_embeddings = get_embeddings(model, qs_test_dataloader)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:47:54.184158Z",
          "iopub.execute_input": "2025-05-05T13:47:54.184446Z",
          "iopub.status.idle": "2025-05-05T13:47:54.738167Z",
          "shell.execute_reply.started": "2025-05-05T13:47:54.184424Z",
          "shell.execute_reply": "2025-05-05T13:47:54.736717Z"
        },
        "id": "AxO2E4k7F3mx",
        "outputId": "e03ffc3d-0300-4341-b4af-10045ee925a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Generating query embeddings...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "submission_items = []\n",
        "\n",
        "for i, q in enumerate(qs_test_embeddings):\n",
        "    for j, d in enumerate(tqdm(doc_embeddings)):\n",
        "        q_id = qs_test[i]['query_id']\n",
        "        doc_id = docs[j]['id']\n",
        "        pred_sim = similarity(d, q)\n",
        "        submission_items.append({\n",
        "            'id': f'{q_id}_{doc_id}',\n",
        "            'doc_id': docs[j]['id'],\n",
        "            'query_id': qs_test[i]['query_id'],\n",
        "            'score': pred_sim\n",
        "        })"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:47:59.363824Z",
          "iopub.execute_input": "2025-05-05T13:47:59.364162Z",
          "iopub.status.idle": "2025-05-05T13:48:03.823726Z",
          "shell.execute_reply.started": "2025-05-05T13:47:59.364135Z",
          "shell.execute_reply": "2025-05-05T13:48:03.822859Z"
        },
        "id": "6VpFElDTF3mx",
        "outputId": "7c2d93fc-e6f1-43ba-a6be-5fb2ea7ebfa3"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 26664/26664 [00:00<00:00, 75569.47it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 93744.43it/s] \n100%|██████████| 26664/26664 [00:00<00:00, 90255.77it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 98708.76it/s] \n100%|██████████| 26664/26664 [00:00<00:00, 94989.89it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 89279.12it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 38564.97it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 95705.89it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 102068.18it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 98527.61it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 98903.68it/s] \n100%|██████████| 26664/26664 [00:00<00:00, 85663.41it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 96748.84it/s]\n100%|██████████| 26664/26664 [00:00<00:00, 99496.03it/s] \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(submission_items)\n",
        "df.set_index('id', inplace=True)\n",
        "df.to_csv('submission.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T11:20:08.758905Z",
          "iopub.execute_input": "2025-05-01T11:20:08.759266Z",
          "iopub.status.idle": "2025-05-01T11:20:29.156702Z",
          "shell.execute_reply.started": "2025-05-01T11:20:08.759233Z",
          "shell.execute_reply": "2025-05-01T11:20:29.155942Z"
        },
        "id": "bhVem7JNF3mx"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}